###### train LLM1 ######
## 얘 하는 일 :             ####
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, pipeline
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import wandb

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
path = './medical/jssp_llm/'
dataset_path = path + "train_llm/dataset_llm1_5k_new_new.jsonl" 
wandb.init(project="jssp-llm", name="jssp_llm_ft")
'''
# 1. 데이터셋 로딩 및 프롬프트 구성
dataset = load_dataset("json", data_files=dataset_path, split="train")

def formatting_func(example):
    return {
        "input": f"{example['instruction']}\n{example['input']}\nOutput:",
        "output": example['output']
    }

dataset = dataset.map(formatting_func)
ds = dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = ds["train"]
eval_dataset = ds["test"]

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype="float16")
model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map="auto")
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

def tokenize_function(examples):
    # input과 output을 붙여 full_texts로
    full_texts = [
        f"{prompt} {target}" for prompt, target in zip(examples["input"], examples["output"])
    ]
    tokenized = tokenizer(
        full_texts,
        truncation=True,
        max_length=512,
        padding="max_length"
    )

    prompt_lens = [
        len(tokenizer(prompt, truncation=True, max_length=512)["input_ids"])
        for prompt in examples["input"]
    ]

    new_labels = []
    for input_ids, p_len in zip(tokenized["input_ids"], prompt_lens):
        input_ids = input_ids[:512]
        lbl = [-100]*p_len + input_ids[p_len:]
        lbl = lbl[:512]
        new_labels.append(lbl)

    tokenized["labels"] = new_labels
    return tokenized

train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    learning_rate=2e-4,
    eval_strategy="steps",
    eval_steps=20,
    save_strategy="steps",
    save_steps=20,
    report_to="wandb",
    logging_steps=10,
    output_dir=path+"/llm1_jssp_mistral7b_lora",
    fp16=True,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

trainer.train()
# test set에서 평가
eval_results = trainer.evaluate()
print("=== Evaluation Results on Test Data ===")
print(eval_results)
trainer.save_model(path+"llm1_jssp_mistral7b_lora_final")
print("Fine-tuning complete and model saved!")
'''
# 2. Prediction 예시 (inference)
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

finetuned_model_path = path + "llm1_jssp_mistral7b_lora_final"
tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)
model = AutoModelForCausalLM.from_pretrained(finetuned_model_path, device_map="auto", trust_remote_code=True)

# 🔵 파이프라인: 디코딩 컨트롤 세팅!
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    do_sample=False,
    repetition_penalty=1.2,
    temperature=0.7,
    eos_token_id=tokenizer.eos_token_id,
)

# 프롬프트 예시 (실제와 동일하게!)
instruction = (
    "Convert the following job description into a matrix. Each row is a job. Each tuple is (machine_index, duration). "
    "Also, return the correct label based on the evaluation criterion. Output must be in JSON format with keys 'matrix' and 'label'."
)
input_text = (
    "Evaluation criterion: Choose the solution generated by the solver that achieved the shortest makespan among all given solvers.\n\n"
    "Job descriptions:\n"
    "Job 1: M3 for 2; then M1 for 3\n"
    "Job 2: M2 for 4"
)
full_prompt = f"{instruction}\n{input_text}\nOutput:"

result = pipe(full_prompt)[0]['generated_text']

print("\n=== 예시 입력 ===")
print(full_prompt)
print("\n=== LLM Prediction ===")
print(result)

# JSON 파싱
import json
try:
    # 마지막 } 기준으로 자름
    start_idx = result.find('{')
    end_idx = result.rfind('}')
    if start_idx != -1 and end_idx != -1:
        json_substr = result[start_idx:end_idx+1]
        parsed = json.loads(json_substr)
        print("\n=== Parsed Output ===")
        print(parsed)
    else:
        print("\n[Warning] JSON 중괄호 블록을 찾지 못함.")
except Exception as e:
    print("\n[Warning] output 파싱 실패!", e)
