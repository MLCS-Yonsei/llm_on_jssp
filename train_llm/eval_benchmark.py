# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ê²½ê³  ë©”ì‹œì§€ ì–µì œ
from transformers import logging
logging.set_verbosity_error()

# 2) transformers ë‚´ë¶€ ë³‘ë ¬í™” ê²€ì‚¬ ìš°íšŒ
import transformers.modeling_utils as _mutils
_mutils.ALL_PARALLEL_STYLES = []

# 3) ëª¨ë“  PreTrainedModel.post_init ì„ ë¹ˆ í•¨ìˆ˜ë¡œ ë®ì–´ì“°ê¸°
from transformers.modeling_utils import PreTrainedModel
PreTrainedModel.post_init = lambda self: None

# 4) T5LayerNorm Apex ì˜¤ë¥˜ íšŒí”¼ìš© íŒ¨ì¹˜
import importlib
import torch
try:
    t5_mod = importlib.import_module("transformers.models.t5.modeling_t5")
    t5_mod.T5LayerNorm = torch.nn.LayerNorm
except ImportError:
    pass
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import os
import json
import time
import pandas as pd
import torch
from tqdm import tqdm
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    pipeline,
)

def load_llm_model(local_path):
    """LoRA ì–´ëŒ‘í„° ëª¨ë¸ê³¼ ì¼ë°˜ LLMì„ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤."""
    # ë‹¨ì¼ GPU ë˜ëŠ” CPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
    device_map = {"": 0} if torch.cuda.is_available() else None

    # adapter_config.json ì´ ìˆìœ¼ë©´ PEFT ë°©ì‹
    if os.path.exists(os.path.join(local_path, "adapter_config.json")):
        from peft import PeftModel, PeftConfig
        adapter_dir = local_path
        config = PeftConfig.from_pretrained(adapter_dir)

        # 1) base modelì„ single-device ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path,
            device_map=None,
            trust_remote_code=True,
        )
        base_model.config.model_parallel = False

        # 2) PEFT ì–´ëŒ‘í„° ì ìš© (device_map ì§€ì • ê°€ëŠ¥)
        model = PeftModel.from_pretrained(
            base_model,
            adapter_dir,
            device_map=device_map,
        )
        tokenizer = AutoTokenizer.from_pretrained(
            config.base_model_name_or_path,
            use_fast=True,
            trust_remote_code=True,
        )
    else:
        # ì¼ë°˜ base model ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(local_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            local_path,
            device_map=device_map,
            trust_remote_code=True,
        )
        model.config.model_parallel = False

    # pipeline ìƒì„± (device ì¸ìëŠ” ì œê±°)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        do_sample=False,
    )
    return pipe

def load_seq2seq_model(model_name):
    """Seq2Seq ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ë¡œë“œí•˜ê³  ë‹¨ì¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•©ë‹ˆë‹¤."""
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # GPUê°€ ìˆìœ¼ë©´ GPUë¡œ, ì—†ìœ¼ë©´ CPUë¡œ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    pipe = pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0 if device.type == "cuda" else -1,
        max_new_tokens=256,
    )
    return pipe

# ====== í‰ê°€ ê¸°ì¤€ í•¨ìˆ˜ë“¤ ======
def exact_match(pred, gt):
    return pred == gt

def matrix_match(pred, gt):
    try:
        return pred["matrix"] == gt["matrix"]
    except:
        return False

def label_match(pred, gt):
    try:
        return pred["label"] == gt["label"]
    except:
        return False

def parse_json_output(output):
    try:
        start = output.find("{")
        end = output.rfind("}")
        if start != -1 and end != -1:
            return json.loads(output[start : end + 1])
    except:
        return None
    return None

# ====== ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í•¨ìˆ˜ ======
def benchmark(models, test_samples):
    result_rows = []
    for mname, model_fn in models.items():
        print(f"\nğŸ” Evaluating model: {mname}")
        EM = MatrixOnly = LabelOnly = Fail = 0
        Times = []

        for sample in tqdm(test_samples, desc=mname):
            prompt = f"{sample['instruction']}\n{sample['input']}\nOutput:"
            gt = json.loads(sample["output"])
            t0 = time.time()
            try:
                output = model_fn(prompt)[0]["generated_text"]
            except Exception:
                output = ""
            t1 = time.time()

            Times.append(t1 - t0)
            pred = parse_json_output(output) if output else None
            if pred is None:
                Fail += 1
                continue
            if exact_match(pred, gt):
                EM += 1
            if matrix_match(pred, gt):
                MatrixOnly += 1
            if label_match(pred, gt):
                LabelOnly += 1

        total = len(test_samples)
        result_rows.append(
            {
                "Model": mname,
                "EM Acc": f"{EM/total:.2%}",
                "Matrix Only": f"{MatrixOnly/total:.2%}",
                "Label Only": f"{LabelOnly/total:.2%}",
                "Format Fail": Fail,
                "Avg Inference Time (s)": round(sum(Times) / total, 3),
            }
        )

    df = pd.DataFrame(result_rows)
    print(df.to_string(index=False))
    return df

# ====== ë©”ì¸ ì‹¤í–‰ ======
if __name__ == "__main__":
    finetuned_llm = load_llm_model(
        "./medical/jssp_llm/llm1_jssp_mistral7b_lora_final"
    )
    mistral_original = load_llm_model("mistralai/Mistral-7B-Instruct-v0.2")
    seq2seq = load_seq2seq_model("t5-base")

    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì„ 5ê°œë¡œ ì œí•œí•˜ì—¬ ë¹ ë¥¸ ì‹¤í–‰
    with open(
        "./medical/jssp_llm/train_llm/test_data_llm1_100.jsonl"
    ) as f:
        test_samples = [json.loads(line) for line in f][:50]

    benchmark(
        {
            "finetuned_llm": lambda prompt: finetuned_llm(prompt),
            "mistral_original": lambda prompt: mistral_original(prompt),
            "seq2seq": lambda prompt: seq2seq(prompt),
        },
        test_samples,
    )
